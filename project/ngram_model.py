from lm_eval.base import LM
from collections import defaultdict
import math
import random
import pickle
from typing import List, Tuple
import numpy as np
from tqdm import tqdm

class NGram(LM):

    def __init__(self, n=3, vocab=None):
        super().__init__()
        self.n = n
        self.ngram_counts = defaultdict(int)
        self.context_counts = defaultdict(int)
#         self.vocab_size = len(self.vocab)
        self.vocab = set()

    def train(self, train_data):
        print(f'Training {self.n}-gram model')
        
        for sentence in tqdm(train_data):
            sentence = sentence['input_ids']
            for i in range(len(sentence) - self.n + 1):
                ngram = tuple(sentence[i:i + self.n])
                context = ngram[:-1]
                self.ngram_counts[ngram] += 1
                self.context_counts[context] += 1
                self.vocab.add(sentence[i])
                            
        self.vocab_size = len(self.vocab)


    def get_ngram_prob(self, ngram):
        context = ngram[:-1]
        return (self.ngram_counts[tuple(ngram)] + 1) / (self.context_counts[tuple(context)] + len(self.vocab))

    def get_cond_prob(self, word, context):
        ngram = tuple(list(context) + [word])
        return self.get_ngram_prob(ngram)

    def get_sent_prob(self, sentence):
        prob = 1
        sentence = sentence['input_ids']
        for i in range(len(sentence) - self.n + 1):
            ngram = tuple(sentence[i:i + self.n])
            prob *= self.get_ngram_prob(ngram)
        return prob

    def get_log_prob(self, sentence):
        prob = 0
        for i in range(len(sentence) - self.n + 1):
            ngram = tuple(sentence[i:i + self.n])
            prob += math.log(self.get_ngram_prob(ngram))
        return prob

    def get_perplexity(self, sentence):
        return math.exp(-self.get_log_prob(sentence) / len(sentence))

    def generate_sent(self):
        sent = ["<s>"] * (self.n - 1)
        while sent[-1] != "</s>":
            context = tuple(sent[-(self.n - 1):])
            sent.append(self.generate_word(context))
        return sent

    def generate_word(self, context):
        r = random.random()
        prob = 0
        for word in self.vocab:
            prob += self.get_cond_prob(word, context)
            if prob > r:
                return word
        return word

    def save_model(self, path):
        with open(path, 'wb') as f:
            pickle.dump(self, f)

    def load_model(path):
        with open(path, 'rb') as f:
            return pickle.load(f)


    # below: abstract methods of LM that need to be implemented. 
    # expected behavior given in documentation
       
    def loglikelihood(self, requests: List[Tuple[str, str]]) -> List[Tuple[float, bool]]:
        """Compute log-likelihood of generating a continuation from a context.
        Downstream tasks should attempt to use loglikelihood instead of other
        LM calls whenever possible.

        Args:
            requests (List[Tuple[str, str]]):
                A list of pairs (context, continuation):
                context (str):
                    Context string. Implementations of LM must be able to handle
                    an empty context string.
                continuation (str):
                    The continuation over which log likelihood will be calculated.
                    If there is a word boundary, the space should be in the
                    continuation. For example, context="hello" continuation=" world"
                    is correct.

        Returns:
            A list of pairs (logprob, isgreedy):
                logprob (float):
                    The log probability of `continuation`.
                isgreedy (bool):
                    Whether `continuation` would be generated by greedy
                    sampling from `context`.
        """

        loglikelihoods = []
        for context, continuation in requests:
            context = context.split()
            continuation = continuation.split()
            loglikelihood = 0
            for i in range(len(continuation)):
                word = continuation[i]
                context = context[-(self.n - 1):]
                context.append(word)
                context = tuple(context)
                loglikelihood += math.log(self.get_cond_prob(word, context))
            loglikelihoods.append((loglikelihood, True))
        return loglikelihoods            


    def loglikelihood_rolling(self, requests: List[Tuple[str, str]]) -> List[float]:
        """Compute full log-likelihood of a string, with no truncation, for perplexity computation
        - We will use the full max context length of the model.
        - For inputs that exceed the max context length, we divide the tokenized string into chunks of up to
        the max context length.
        - IMPORTANT: Each document's loglikelihood/perplexity is computed *separately*, unlike other implementations
        which may simply concatenate multiple documents together.
        - IMPORTANT: We maximize the amount of context for each prediction. Specifically, for inputs that we break into
        multiple chunks, the last input will still a full-sized context.
        Example:
            Input tokens: [ 0 1 2 3 4 5 6 7 8 9 ]
            Prefix: EOT
            Max context length: 4
            Resulting input/prediction pairs:

                INPUT:  EOT   0   1   2
                PRED:     0   1   2   3

                INPUT:    3   4   5   6
                PRED:     4   5   6   7

                INPUT:    5   6   7   8
                PRED:             8   9

        Observe that:
            1. Each token is predicted exactly once
            2. For the last pair, we provide the full context, but only score the last two tokens

        Args:
            requests (List[Tuple[str, str]]):
                A list of paired strings.
                string (str):
                    String for which we are computing per-token loglikelihood.

        Returns:
            A list of logprobs on the `continuation`.
        """
        
        loglikelihoods = []
        for context, continuation in requests:
            context = context.split()
            continuation = continuation.split()
            loglikelihood = 0
            for i in range(len(continuation)):
                word = continuation[i]
                context = context[-(self.n - 1):]
                context.append(word)
                context = tuple(context)
                loglikelihood += math.log(self.get_cond_prob(word, context))
            loglikelihoods.append(loglikelihood)
        return loglikelihoods


    def greedy_until(self, requests: List[Tuple[str, dict]]) -> List[str]:
        """Generate greedily until a stopping sequence or max generation length.

        Args:
            requests (List[Tuple[str, dict]]):
                A list of pairs (context, args):
                context (str):
                    Context string.
                args (dict):
                    A dictionary of generation arguments in the form:
                    {
                        stop_sequences: str,
                        max_generation_length: int,
                        num_fewshot: int
                    }

        Returns:
            A list of strings continuation:
                continuation: str
                    The generated continuation.
        """
        continuations = []
        for context, args in requests:
            context = context.split()
            stop_sequences = args["stop_sequences"]
            max_generation_length = args["max_generation_length"]
            num_fewshot = args["num_fewshot"]
            continuation = []
            for i in range(max_generation_length):
                word = self.generate_word(tuple(context[-(self.n - 1):]))
                continuation.append(word)
                context.append(word)
                if word in stop_sequences:
                    break
            continuations.append(" ".join(continuation))
        return continuations