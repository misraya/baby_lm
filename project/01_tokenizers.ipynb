{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/misra/anaconda3/envs/babylm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "os.chdir('..')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing pretrained tokenizers used in baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, GPT2Tokenizer, RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Tokenizer(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True)})\n"
     ]
    }
   ],
   "source": [
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "print(gpt2_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer inputs: ['input_ids', 'attention_mask']\n",
      "Tokenization method: tokenize\n",
      "Special tokens: {'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenizer inputs:\", gpt2_tokenizer.model_input_names)\n",
    "print(\"Tokenization method:\", gpt2_tokenizer.tokenize.__name__)\n",
    "print(\"Special tokens:\", gpt2_tokenizer.special_tokens_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaTokenizer(name_or_path='roberta-base', vocab_size=50265, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'sep_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'cls_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True)})\n"
     ]
    }
   ],
   "source": [
    "roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "print(roberta_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer inputs: ['input_ids', 'attention_mask']\n",
      "Tokenization method: tokenize\n",
      "Special tokens: {'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenizer inputs:\", roberta_tokenizer.model_input_names)\n",
    "print(\"Tokenization method:\", roberta_tokenizer.tokenize.__name__)\n",
    "print(\"Special tokens:\", roberta_tokenizer.special_tokens_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Tokenizer(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<image>', '</c>', '<PERSON>']})\n"
     ]
    }
   ],
   "source": [
    "opt_tokenizer = GPT2Tokenizer.from_pretrained(\"facebook/opt-125m\")\n",
    "opt_tokenizer.add_bos_token = False\n",
    "opt_tokenizer.add_special_tokens({\n",
    "            'bos_token': '<s>', \n",
    "            'eos_token': '</s>',\n",
    "            'unk_token': '<unk>',\n",
    "            'pad_token': '<pad>',\n",
    "            'additional_special_tokens': [\n",
    "                '<image>', '</c>', \n",
    "                '<PERSON>', # C-12M for person names\n",
    "                ]})\n",
    "\n",
    "print(opt_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer inputs: ['input_ids', 'attention_mask']\n",
      "Tokenization method: tokenize\n",
      "Special tokens: {'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<image>', '</c>', '<PERSON>']}\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenizer inputs:\", opt_tokenizer.model_input_names)\n",
    "print(\"Tokenization method:\", opt_tokenizer.tokenize.__name__)\n",
    "print(\"Special tokens:\", opt_tokenizer.special_tokens_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trying an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text from https://huggingface.co/docs/transformers/tokenizer_summary\n",
    "text = 'As we saw in the preprocessing tutorial, tokenizing a text is splitting it into words or subwords, which then are converted to ids through a look-up table. Converting words or subwords to ids is straightforward, so in this summary, we will focus on splitting a text into words or subwords (i.e. tokenizing a text). More specifically, we will look at the three main types of tokenizers used in 🤗 Transformers: Byte-Pair Encoding (BPE), WordPiece, and SentencePiece, and show examples of which tokenizer type is used by which model.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 1722,   356,  2497,   287,   262,   662, 36948, 11808,    11, 11241,\n",
      "          2890,   257,  2420,   318, 26021,   340,   656,  2456,   393,   850,\n",
      "         10879,    11,   543,   788,   389, 11513,   284,   220,  2340,   832,\n",
      "           257,   804,    12,   929,  3084,    13, 35602,   889,  2456,   393,\n",
      "           850, 10879,   284,   220,  2340,   318, 15836,    11,   523,   287,\n",
      "           428, 10638,    11,   356,   481,  2962,   319, 26021,   257,  2420,\n",
      "           656,  2456,   393,   850, 10879,   357,    72,    13,    68,    13,\n",
      "         11241,  2890,   257,  2420,   737,  3125,  5734,    11,   356,   481,\n",
      "           804,   379,   262,  1115,  1388,  3858,   286, 11241, 11341,   973,\n",
      "           287, 12520,    97,   245, 39185,    25, 30589,    12,    47,   958,\n",
      "         14711,  7656,   357,    33, 11401,   828,  9678,    47,  8535,    11,\n",
      "           290, 11352,   594,    47,  8535,    11,   290,   905,  6096,   286,\n",
      "           543, 11241,  7509,  2099,   318,   973,   416,   543,  2746,    13]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "['As', 'Ġwe', 'Ġsaw', 'Ġin', 'Ġthe', 'Ġpre', 'processing', 'Ġtutorial', ',', 'Ġtoken', 'izing', 'Ġa', 'Ġtext', 'Ġis', 'Ġsplitting', 'Ġit', 'Ġinto', 'Ġwords', 'Ġor', 'Ġsub', 'words', ',', 'Ġwhich', 'Ġthen', 'Ġare', 'Ġconverted', 'Ġto', 'Ġ', 'ids', 'Ġthrough', 'Ġa', 'Ġlook', '-', 'up', 'Ġtable', '.', 'ĠConver', 'ting', 'Ġwords', 'Ġor', 'Ġsub', 'words', 'Ġto', 'Ġ', 'ids', 'Ġis', 'Ġstraightforward', ',', 'Ġso', 'Ġin', 'Ġthis', 'Ġsummary', ',', 'Ġwe', 'Ġwill', 'Ġfocus', 'Ġon', 'Ġsplitting', 'Ġa', 'Ġtext', 'Ġinto', 'Ġwords', 'Ġor', 'Ġsub', 'words', 'Ġ(', 'i', '.', 'e', '.', 'Ġtoken', 'izing', 'Ġa', 'Ġtext', ').', 'ĠMore', 'Ġspecifically', ',', 'Ġwe', 'Ġwill', 'Ġlook', 'Ġat', 'Ġthe', 'Ġthree', 'Ġmain', 'Ġtypes', 'Ġof', 'Ġtoken', 'izers', 'Ġused', 'Ġin', 'ĠðŁ', '¤', 'Ĺ', 'ĠTransformers', ':', 'ĠByte', '-', 'P', 'air', 'ĠEnc', 'oding', 'Ġ(', 'B', 'PE', '),', 'ĠWord', 'P', 'iece', ',', 'Ġand', 'ĠSent', 'ence', 'P', 'iece', ',', 'Ġand', 'Ġshow', 'Ġexamples', 'Ġof', 'Ġwhich', 'Ġtoken', 'izer', 'Ġtype', 'Ġis', 'Ġused', 'Ġby', 'Ġwhich', 'Ġmodel', '.']\n"
     ]
    }
   ],
   "source": [
    "inputs = gpt2_tokenizer([text], padding=False, truncation=False, return_tensors=\"pt\")\n",
    "print(inputs)\n",
    "print(gpt2_tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0,  1620,    52,   794,    11,     5,  1198, 39221, 35950,     6,\n",
      "         19233,  2787,    10,  2788,    16, 21128,    24,    88,  1617,    50,\n",
      "          2849, 30938,     6,    61,   172,    32,  8417,     7,  1437,  7823,\n",
      "           149,    10,   356,    12,   658,  2103,     4, 36608,  2577,  1617,\n",
      "            50,  2849, 30938,     7,  1437,  7823,    16, 15196,     6,    98,\n",
      "            11,    42,  4819,     6,    52,    40,  1056,    15, 21128,    10,\n",
      "          2788,    88,  1617,    50,  2849, 30938,    36,   118,     4,   242,\n",
      "             4, 19233,  2787,    10,  2788,   322,   901,  4010,     6,    52,\n",
      "            40,   356,    23,     5,   130,  1049,  3505,     9, 19233, 11574,\n",
      "           341,    11,  8103, 10470,  6800, 34379,    35, 46594,    12,   510,\n",
      "          2456, 14813, 19519,    36,   387, 16035,   238, 15690,   510, 39426,\n",
      "             6,     8, 12169,  4086,   510, 39426,     6,     8,   311,  7721,\n",
      "             9,    61, 19233,  6315,  1907,    16,   341,    30,    61,  1421,\n",
      "             4,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "['As', 'Ġwe', 'Ġsaw', 'Ġin', 'Ġthe', 'Ġpre', 'processing', 'Ġtutorial', ',', 'Ġtoken', 'izing', 'Ġa', 'Ġtext', 'Ġis', 'Ġsplitting', 'Ġit', 'Ġinto', 'Ġwords', 'Ġor', 'Ġsub', 'words', ',', 'Ġwhich', 'Ġthen', 'Ġare', 'Ġconverted', 'Ġto', 'Ġ', 'ids', 'Ġthrough', 'Ġa', 'Ġlook', '-', 'up', 'Ġtable', '.', 'ĠConver', 'ting', 'Ġwords', 'Ġor', 'Ġsub', 'words', 'Ġto', 'Ġ', 'ids', 'Ġis', 'Ġstraightforward', ',', 'Ġso', 'Ġin', 'Ġthis', 'Ġsummary', ',', 'Ġwe', 'Ġwill', 'Ġfocus', 'Ġon', 'Ġsplitting', 'Ġa', 'Ġtext', 'Ġinto', 'Ġwords', 'Ġor', 'Ġsub', 'words', 'Ġ(', 'i', '.', 'e', '.', 'Ġtoken', 'izing', 'Ġa', 'Ġtext', ').', 'ĠMore', 'Ġspecifically', ',', 'Ġwe', 'Ġwill', 'Ġlook', 'Ġat', 'Ġthe', 'Ġthree', 'Ġmain', 'Ġtypes', 'Ġof', 'Ġtoken', 'izers', 'Ġused', 'Ġin', 'ĠðŁ', '¤', 'Ĺ', 'ĠTransformers', ':', 'ĠByte', '-', 'P', 'air', 'ĠEnc', 'oding', 'Ġ(', 'B', 'PE', '),', 'ĠWord', 'P', 'iece', ',', 'Ġand', 'ĠSent', 'ence', 'P', 'iece', ',', 'Ġand', 'Ġshow', 'Ġexamples', 'Ġof', 'Ġwhich', 'Ġtoken', 'izer', 'Ġtype', 'Ġis', 'Ġused', 'Ġby', 'Ġwhich', 'Ġmodel', '.']\n"
     ]
    }
   ],
   "source": [
    "inputs = roberta_tokenizer([text], padding=False, truncation=False, return_tensors=\"pt\")\n",
    "print(inputs)\n",
    "print(roberta_tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 1620,    52,   794,    11,     5,  1198, 39221, 35950,     6, 19233,\n",
      "          2787,    10,  2788,    16, 21128,    24,    88,  1617,    50,  2849,\n",
      "         30938,     6,    61,   172,    32,  8417,     7,  1437,  7823,   149,\n",
      "            10,   356,    12,   658,  2103,     4, 36608,  2577,  1617,    50,\n",
      "          2849, 30938,     7,  1437,  7823,    16, 15196,     6,    98,    11,\n",
      "            42,  4819,     6,    52,    40,  1056,    15, 21128,    10,  2788,\n",
      "            88,  1617,    50,  2849, 30938,    36,   118,     4,   242,     4,\n",
      "         19233,  2787,    10,  2788,   322,   901,  4010,     6,    52,    40,\n",
      "           356,    23,     5,   130,  1049,  3505,     9, 19233, 11574,   341,\n",
      "            11,  8103, 10470,  6800, 34379,    35, 46594,    12,   510,  2456,\n",
      "         14813, 19519,    36,   387, 16035,   238, 15690,   510, 39426,     6,\n",
      "             8, 12169,  4086,   510, 39426,     6,     8,   311,  7721,     9,\n",
      "            61, 19233,  6315,  1907,    16,   341,    30,    61,  1421,     4]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "['As', 'Ġwe', 'Ġsaw', 'Ġin', 'Ġthe', 'Ġpre', 'processing', 'Ġtutorial', ',', 'Ġtoken', 'izing', 'Ġa', 'Ġtext', 'Ġis', 'Ġsplitting', 'Ġit', 'Ġinto', 'Ġwords', 'Ġor', 'Ġsub', 'words', ',', 'Ġwhich', 'Ġthen', 'Ġare', 'Ġconverted', 'Ġto', 'Ġ', 'ids', 'Ġthrough', 'Ġa', 'Ġlook', '-', 'up', 'Ġtable', '.', 'ĠConver', 'ting', 'Ġwords', 'Ġor', 'Ġsub', 'words', 'Ġto', 'Ġ', 'ids', 'Ġis', 'Ġstraightforward', ',', 'Ġso', 'Ġin', 'Ġthis', 'Ġsummary', ',', 'Ġwe', 'Ġwill', 'Ġfocus', 'Ġon', 'Ġsplitting', 'Ġa', 'Ġtext', 'Ġinto', 'Ġwords', 'Ġor', 'Ġsub', 'words', 'Ġ(', 'i', '.', 'e', '.', 'Ġtoken', 'izing', 'Ġa', 'Ġtext', ').', 'ĠMore', 'Ġspecifically', ',', 'Ġwe', 'Ġwill', 'Ġlook', 'Ġat', 'Ġthe', 'Ġthree', 'Ġmain', 'Ġtypes', 'Ġof', 'Ġtoken', 'izers', 'Ġused', 'Ġin', 'ĠðŁ', '¤', 'Ĺ', 'ĠTransformers', ':', 'ĠByte', '-', 'P', 'air', 'ĠEnc', 'oding', 'Ġ(', 'B', 'PE', '),', 'ĠWord', 'P', 'iece', ',', 'Ġand', 'ĠSent', 'ence', 'P', 'iece', ',', 'Ġand', 'Ġshow', 'Ġexamples', 'Ġof', 'Ġwhich', 'Ġtoken', 'izer', 'Ġtype', 'Ġis', 'Ġused', 'Ġby', 'Ġwhich', 'Ġmodel', '.']\n"
     ]
    }
   ],
   "source": [
    "inputs = opt_tokenizer([text], padding=False, truncation=False, return_tensors=\"pt\")\n",
    "print(inputs)\n",
    "print(opt_tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_tokenizer.tokenize(text) == roberta_tokenizer.tokenize(text) == opt_tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the tokenizer pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As we saw in the preprocessing tutorial, tokenizing a text is splitting it into words or subwords, which then are converted to ids through a look-up table. Converting words or subwords to ids is straightforward, so in this summary, we will focus on splitting a text into words or subwords (i.e. tokenizing a text). More specifically, we will look at the three main types of tokenizers used in 🤗 Transformers: Byte-Pair Encoding (BPE), WordPiece, and SentencePiece, and show examples of which tokenizer type is used by which model.\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import NFD, StripAccents\n",
    "normalizer = normalizers.Sequence([NFD(), StripAccents()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As we saw in the preprocessing tutorial, tokenizing a text is splitting it into words or subwords, which then are converted to ids through a look-up table. Converting words or subwords to ids is straightforward, so in this summary, we will focus on splitting a text into words or subwords (i.e. tokenizing a text). More specifically, we will look at the three main types of tokenizers used in 🤗 Transformers: Byte-Pair Encoding (BPE), WordPiece, and SentencePiece, and show examples of which tokenizer type is used by which model.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizer.normalize_str(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello how are u?'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizer.normalize_str(\"Héllò hôw are ü?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('As', (0, 2)),\n",
       " ('we', (3, 5)),\n",
       " ('saw', (6, 9)),\n",
       " ('in', (10, 12)),\n",
       " ('the', (13, 16)),\n",
       " ('preprocessing', (17, 30)),\n",
       " ('tutorial', (31, 39)),\n",
       " (',', (39, 40)),\n",
       " ('tokenizing', (41, 51)),\n",
       " ('a', (52, 53)),\n",
       " ('text', (54, 58)),\n",
       " ('is', (59, 61)),\n",
       " ('splitting', (62, 71)),\n",
       " ('it', (72, 74)),\n",
       " ('into', (75, 79)),\n",
       " ('words', (80, 85)),\n",
       " ('or', (86, 88)),\n",
       " ('subwords', (89, 97)),\n",
       " (',', (97, 98)),\n",
       " ('which', (99, 104)),\n",
       " ('then', (105, 109)),\n",
       " ('are', (110, 113)),\n",
       " ('converted', (114, 123)),\n",
       " ('to', (124, 126)),\n",
       " ('ids', (127, 130)),\n",
       " ('through', (131, 138)),\n",
       " ('a', (139, 140)),\n",
       " ('look', (141, 145)),\n",
       " ('-', (145, 146)),\n",
       " ('up', (146, 148)),\n",
       " ('table', (149, 154)),\n",
       " ('.', (154, 155)),\n",
       " ('Converting', (156, 166)),\n",
       " ('words', (167, 172)),\n",
       " ('or', (173, 175)),\n",
       " ('subwords', (176, 184)),\n",
       " ('to', (185, 187)),\n",
       " ('ids', (188, 191)),\n",
       " ('is', (192, 194)),\n",
       " ('straightforward', (195, 210)),\n",
       " (',', (210, 211)),\n",
       " ('so', (212, 214)),\n",
       " ('in', (215, 217)),\n",
       " ('this', (218, 222)),\n",
       " ('summary', (223, 230)),\n",
       " (',', (230, 231)),\n",
       " ('we', (232, 234)),\n",
       " ('will', (235, 239)),\n",
       " ('focus', (240, 245)),\n",
       " ('on', (246, 248)),\n",
       " ('splitting', (249, 258)),\n",
       " ('a', (259, 260)),\n",
       " ('text', (261, 265)),\n",
       " ('into', (266, 270)),\n",
       " ('words', (271, 276)),\n",
       " ('or', (277, 279)),\n",
       " ('subwords', (280, 288)),\n",
       " ('(', (289, 290)),\n",
       " ('i', (290, 291)),\n",
       " ('.', (291, 292)),\n",
       " ('e', (292, 293)),\n",
       " ('.', (293, 294)),\n",
       " ('tokenizing', (295, 305)),\n",
       " ('a', (306, 307)),\n",
       " ('text', (308, 312)),\n",
       " (').', (312, 314)),\n",
       " ('More', (315, 319)),\n",
       " ('specifically', (320, 332)),\n",
       " (',', (332, 333)),\n",
       " ('we', (334, 336)),\n",
       " ('will', (337, 341)),\n",
       " ('look', (342, 346)),\n",
       " ('at', (347, 349)),\n",
       " ('the', (350, 353)),\n",
       " ('three', (354, 359)),\n",
       " ('main', (360, 364)),\n",
       " ('types', (365, 370)),\n",
       " ('of', (371, 373)),\n",
       " ('tokenizers', (374, 384)),\n",
       " ('used', (385, 389)),\n",
       " ('in', (390, 392)),\n",
       " ('🤗', (393, 394)),\n",
       " ('Transformers', (395, 407)),\n",
       " (':', (407, 408)),\n",
       " ('Byte', (409, 413)),\n",
       " ('-', (413, 414)),\n",
       " ('Pair', (414, 418)),\n",
       " ('Encoding', (419, 427)),\n",
       " ('(', (428, 429)),\n",
       " ('BPE', (429, 432)),\n",
       " ('),', (432, 434)),\n",
       " ('WordPiece', (435, 444)),\n",
       " (',', (444, 445)),\n",
       " ('and', (446, 449)),\n",
       " ('SentencePiece', (450, 463)),\n",
       " (',', (463, 464)),\n",
       " ('and', (465, 468)),\n",
       " ('show', (469, 473)),\n",
       " ('examples', (474, 482)),\n",
       " ('of', (483, 485)),\n",
       " ('which', (486, 491)),\n",
       " ('tokenizer', (492, 501)),\n",
       " ('type', (502, 506)),\n",
       " ('is', (507, 509)),\n",
       " ('used', (510, 514)),\n",
       " ('by', (515, 517)),\n",
       " ('which', (518, 523)),\n",
       " ('model', (524, 529)),\n",
       " ('.', (529, 530))]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "pre_tokenizer = Whitespace()\n",
    "pre_tokenizer.pre_tokenize_str(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('As', (0, 2)),\n",
       " ('we', (3, 5)),\n",
       " ('saw', (6, 9)),\n",
       " ('in', (10, 12)),\n",
       " ('the', (13, 16)),\n",
       " ('preprocessing', (17, 30)),\n",
       " ('tutorial', (31, 39)),\n",
       " (',', (39, 40)),\n",
       " ('tokenizing', (41, 51)),\n",
       " ('a', (52, 53)),\n",
       " ('text', (54, 58)),\n",
       " ('is', (59, 61)),\n",
       " ('splitting', (62, 71)),\n",
       " ('it', (72, 74)),\n",
       " ('into', (75, 79)),\n",
       " ('words', (80, 85)),\n",
       " ('or', (86, 88)),\n",
       " ('subwords', (89, 97)),\n",
       " (',', (97, 98)),\n",
       " ('which', (99, 104)),\n",
       " ('then', (105, 109)),\n",
       " ('are', (110, 113)),\n",
       " ('converted', (114, 123)),\n",
       " ('to', (124, 126)),\n",
       " ('ids', (127, 130)),\n",
       " ('through', (131, 138)),\n",
       " ('a', (139, 140)),\n",
       " ('look', (141, 145)),\n",
       " ('-', (145, 146)),\n",
       " ('up', (146, 148)),\n",
       " ('table', (149, 154)),\n",
       " ('.', (154, 155)),\n",
       " ('Converting', (156, 166)),\n",
       " ('words', (167, 172)),\n",
       " ('or', (173, 175)),\n",
       " ('subwords', (176, 184)),\n",
       " ('to', (185, 187)),\n",
       " ('ids', (188, 191)),\n",
       " ('is', (192, 194)),\n",
       " ('straightforward', (195, 210)),\n",
       " (',', (210, 211)),\n",
       " ('so', (212, 214)),\n",
       " ('in', (215, 217)),\n",
       " ('this', (218, 222)),\n",
       " ('summary', (223, 230)),\n",
       " (',', (230, 231)),\n",
       " ('we', (232, 234)),\n",
       " ('will', (235, 239)),\n",
       " ('focus', (240, 245)),\n",
       " ('on', (246, 248)),\n",
       " ('splitting', (249, 258)),\n",
       " ('a', (259, 260)),\n",
       " ('text', (261, 265)),\n",
       " ('into', (266, 270)),\n",
       " ('words', (271, 276)),\n",
       " ('or', (277, 279)),\n",
       " ('subwords', (280, 288)),\n",
       " ('(', (289, 290)),\n",
       " ('i', (290, 291)),\n",
       " ('.', (291, 292)),\n",
       " ('e', (292, 293)),\n",
       " ('.', (293, 294)),\n",
       " ('tokenizing', (295, 305)),\n",
       " ('a', (306, 307)),\n",
       " ('text', (308, 312)),\n",
       " (').', (312, 314)),\n",
       " ('More', (315, 319)),\n",
       " ('specifically', (320, 332)),\n",
       " (',', (332, 333)),\n",
       " ('we', (334, 336)),\n",
       " ('will', (337, 341)),\n",
       " ('look', (342, 346)),\n",
       " ('at', (347, 349)),\n",
       " ('the', (350, 353)),\n",
       " ('three', (354, 359)),\n",
       " ('main', (360, 364)),\n",
       " ('types', (365, 370)),\n",
       " ('of', (371, 373)),\n",
       " ('tokenizers', (374, 384)),\n",
       " ('used', (385, 389)),\n",
       " ('in', (390, 392)),\n",
       " ('🤗', (393, 394)),\n",
       " ('Transformers', (395, 407)),\n",
       " (':', (407, 408)),\n",
       " ('Byte', (409, 413)),\n",
       " ('-', (413, 414)),\n",
       " ('Pair', (414, 418)),\n",
       " ('Encoding', (419, 427)),\n",
       " ('(', (428, 429)),\n",
       " ('BPE', (429, 432)),\n",
       " ('),', (432, 434)),\n",
       " ('WordPiece', (435, 444)),\n",
       " (',', (444, 445)),\n",
       " ('and', (446, 449)),\n",
       " ('SentencePiece', (450, 463)),\n",
       " (',', (463, 464)),\n",
       " ('and', (465, 468)),\n",
       " ('show', (469, 473)),\n",
       " ('examples', (474, 482)),\n",
       " ('of', (483, 485)),\n",
       " ('which', (486, 491)),\n",
       " ('tokenizer', (492, 501)),\n",
       " ('type', (502, 506)),\n",
       " ('is', (507, 509)),\n",
       " ('used', (510, 514)),\n",
       " ('by', (515, 517)),\n",
       " ('which', (518, 523)),\n",
       " ('model', (524, 529)),\n",
       " ('.', (529, 530))]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import pre_tokenizers\n",
    "from tokenizers.pre_tokenizers import Digits\n",
    "pre_tokenizer = pre_tokenizers.Sequence([Whitespace(), Digits(individual_digits=True)])\n",
    "pre_tokenizer.pre_tokenize_str(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.processors import TemplateProcessing\n",
    "gpt2_tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[(\"[CLS]\", 1), (\"[SEP]\", 2)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1722, 356, 2497, 287, 262, 662, 36948, 11808, 11, 11241, 2890, 257, 2420, 318, 26021, 340, 656, 2456, 393, 850, 10879, 11, 543, 788, 389, 11513, 284, 220, 2340, 832, 257, 804, 12, 929, 3084, 13, 35602, 889, 2456, 393, 850, 10879, 284, 220, 2340, 318, 15836, 11, 523, 287, 428, 10638, 11, 356, 481, 2962, 319, 26021, 257, 2420, 656, 2456, 393, 850, 10879, 357, 72, 13, 68, 13, 11241, 2890, 257, 2420, 737, 3125, 5734, 11, 356, 481, 804, 379, 262, 1115, 1388, 3858, 286, 11241, 11341, 973, 287, 12520, 97, 245, 39185, 25, 30589, 12, 47, 958, 14711, 7656, 357, 33, 11401, 828, 9678, 47, 8535, 11, 290, 11352, 594, 47, 8535, 11, 290, 905, 6096, 286, 543, 11241, 7509, 2099, 318, 973, 416, 543, 2746, 13], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As we saw in the preprocessing tutorial, tokenizing a text is splitting it into words or subwords, which then are converted to ids through a look-up table. Converting words or subwords to ids is straightforward, so in this summary, we will focus on splitting a text into words or subwords (i.e. tokenizing a text). More specifically, we will look at the three main types of tokenizers used in 🤗 Transformers: Byte-Pair Encoding (BPE), WordPiece, and SentencePiece, and show examples of which tokenizer type is used by which model.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_tokenizer.decode(gpt2_tokenizer(text)['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_tokenizer.decode(gpt2_tokenizer(text)['input_ids']) == text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a tokenizer from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../datasets/babylm_10M/bnc_spoken.train',\n",
       " '../datasets/babylm_10M/children_stories.train',\n",
       " '../datasets/babylm_10M/cbt.train',\n",
       " '../datasets/babylm_10M/switchboard.train',\n",
       " '../datasets/babylm_10M/wikipedia.train',\n",
       " '../datasets/babylm_10M/gutenberg.train',\n",
       " '../datasets/babylm_10M/aochildes.train',\n",
       " '../datasets/babylm_10M/qed.train',\n",
       " '../datasets/babylm_10M/simple_wikipedia.train',\n",
       " '../datasets/babylm_10M/open_subtitles.train']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths = [str(x) for x in Path(\"../datasets/babylm_10M/\").glob(\"*.train\")]\n",
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset baby_lm_for_hf (/home/misra/.cache/huggingface/datasets/baby_lm_for_hf/babyLM-10M/1.0.0/281c1a7c3ebf0b682e9bdca60f4a2442b6aaf2d2a266fea843461e98f10a5f07)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(path=os.path.join('./src/babylm_baseline_train/datasets', \"babyLM_for_hf.py\"),\n",
    "            name='babyLM-10M',\n",
    "            split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three main tokenizer types: \n",
    "- Byte-Pair Encoding (BPE)\n",
    "- WordPiece\n",
    "- SentencePiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Byte-Pair Encoding Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import NFD, Lowercase, StripAccents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokenizer(vocabulary_size=0, model=ByteLevelBPE, add_prefix_space=False, lowercase=False, dropout=None, unicode_normalizer=None, continuing_subword_prefix=None, end_of_word_suffix=None, trim_offsets=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = ByteLevelBPETokenizer()\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.normalizer = normalizers.Sequence([NFD(), Lowercase(), StripAccents()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tokenizers.normalizers.Sequence at 0x7fc94a0a78f0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tokenizers.pre_tokenizers.ByteLevel at 0x7fc94a0a3470>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pre_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tokenizers.decoders.ByteLevel at 0x7fc94a08a660>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tokenizers.processors.ByteLevel at 0x7fc972f94210>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.post_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train(files=paths, vocab_size=52_000, min_frequency=2, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=126, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['as',\n",
       " 'Ġwe',\n",
       " 'Ġsaw',\n",
       " 'Ġin',\n",
       " 'Ġthe',\n",
       " 'Ġpre',\n",
       " 'process',\n",
       " 'ing',\n",
       " 'Ġtutorial',\n",
       " ',',\n",
       " 'Ġtoken',\n",
       " 'izing',\n",
       " 'Ġa',\n",
       " 'Ġtext',\n",
       " 'Ġis',\n",
       " 'Ġsplitting',\n",
       " 'Ġit',\n",
       " 'Ġinto',\n",
       " 'Ġwords',\n",
       " 'Ġor',\n",
       " 'Ġsub',\n",
       " 'words',\n",
       " ',',\n",
       " 'Ġwhich',\n",
       " 'Ġthen',\n",
       " 'Ġare',\n",
       " 'Ġconverted',\n",
       " 'Ġto',\n",
       " 'Ġids',\n",
       " 'Ġthrough',\n",
       " 'Ġa',\n",
       " 'Ġlook',\n",
       " '-',\n",
       " 'up',\n",
       " 'Ġtable',\n",
       " '.',\n",
       " 'Ġconverting',\n",
       " 'Ġwords',\n",
       " 'Ġor',\n",
       " 'Ġsub',\n",
       " 'words',\n",
       " 'Ġto',\n",
       " 'Ġids',\n",
       " 'Ġis',\n",
       " 'Ġstraightforward',\n",
       " ',',\n",
       " 'Ġso',\n",
       " 'Ġin',\n",
       " 'Ġthis',\n",
       " 'Ġsummary',\n",
       " ',',\n",
       " 'Ġwe',\n",
       " 'Ġwill',\n",
       " 'Ġfocus',\n",
       " 'Ġon',\n",
       " 'Ġsplitting',\n",
       " 'Ġa',\n",
       " 'Ġtext',\n",
       " 'Ġinto',\n",
       " 'Ġwords',\n",
       " 'Ġor',\n",
       " 'Ġsub',\n",
       " 'words',\n",
       " 'Ġ(',\n",
       " 'i',\n",
       " '.',\n",
       " 'e',\n",
       " '.',\n",
       " 'Ġtoken',\n",
       " 'izing',\n",
       " 'Ġa',\n",
       " 'Ġtext',\n",
       " ').',\n",
       " 'Ġmore',\n",
       " 'Ġspecifically',\n",
       " ',',\n",
       " 'Ġwe',\n",
       " 'Ġwill',\n",
       " 'Ġlook',\n",
       " 'Ġat',\n",
       " 'Ġthe',\n",
       " 'Ġthree',\n",
       " 'Ġmain',\n",
       " 'Ġtypes',\n",
       " 'Ġof',\n",
       " 'Ġtoken',\n",
       " 'izers',\n",
       " 'Ġused',\n",
       " 'Ġin',\n",
       " 'Ġ',\n",
       " 'ð',\n",
       " 'Ł',\n",
       " '¤',\n",
       " 'Ĺ',\n",
       " 'Ġtransform',\n",
       " 'ers',\n",
       " ':',\n",
       " 'Ġbyte',\n",
       " '-',\n",
       " 'pair',\n",
       " 'Ġencoding',\n",
       " 'Ġ(',\n",
       " 'b',\n",
       " 'pe',\n",
       " '),',\n",
       " 'Ġword',\n",
       " 'piece',\n",
       " ',',\n",
       " 'Ġand',\n",
       " 'Ġsentence',\n",
       " 'piece',\n",
       " ',',\n",
       " 'Ġand',\n",
       " 'Ġshow',\n",
       " 'Ġexamples',\n",
       " 'Ġof',\n",
       " 'Ġwhich',\n",
       " 'Ġtoken',\n",
       " 'izer',\n",
       " 'Ġtype',\n",
       " 'Ġis',\n",
       " 'Ġused',\n",
       " 'Ġby',\n",
       " 'Ġwhich',\n",
       " 'Ġmodel',\n",
       " '.']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text).tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"tokenizers/babylm_10M_BPE.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordPiece Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordpiece_tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordpiece_tokenizer.normalizer = normalizers.Sequence([NFD(), Lowercase(), StripAccents()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordpiece_tokenizer.pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function Tokenizer.decode(self, ids, skip_special_tokens=True)>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpiece_tokenizer.decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordpiece_tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $0 [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[(\"[CLS]\", 1), (\"[SEP]\", 0)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = WordPieceTrainer(vocab_size=30522, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wordpiece_tokenizer.train(paths, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=126, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpiece_tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'as',\n",
       " 'we',\n",
       " 'saw',\n",
       " 'in',\n",
       " 'the',\n",
       " 'prep',\n",
       " '##rocess',\n",
       " '##ing',\n",
       " 'tutorial',\n",
       " ',',\n",
       " 'token',\n",
       " '##izing',\n",
       " 'a',\n",
       " 'text',\n",
       " 'is',\n",
       " 'splitting',\n",
       " 'it',\n",
       " 'into',\n",
       " 'words',\n",
       " 'or',\n",
       " 'sub',\n",
       " '##words',\n",
       " ',',\n",
       " 'which',\n",
       " 'then',\n",
       " 'are',\n",
       " 'converted',\n",
       " 'to',\n",
       " 'ids',\n",
       " 'through',\n",
       " 'a',\n",
       " 'look',\n",
       " '-',\n",
       " 'up',\n",
       " 'table',\n",
       " '.',\n",
       " 'converting',\n",
       " 'words',\n",
       " 'or',\n",
       " 'sub',\n",
       " '##words',\n",
       " 'to',\n",
       " 'ids',\n",
       " 'is',\n",
       " 'straightforward',\n",
       " ',',\n",
       " 'so',\n",
       " 'in',\n",
       " 'this',\n",
       " 'summary',\n",
       " ',',\n",
       " 'we',\n",
       " 'will',\n",
       " 'focus',\n",
       " 'on',\n",
       " 'splitting',\n",
       " 'a',\n",
       " 'text',\n",
       " 'into',\n",
       " 'words',\n",
       " 'or',\n",
       " 'sub',\n",
       " '##words',\n",
       " '(',\n",
       " 'i',\n",
       " '.',\n",
       " 'e',\n",
       " '.',\n",
       " 'token',\n",
       " '##izing',\n",
       " 'a',\n",
       " 'text',\n",
       " ').',\n",
       " 'more',\n",
       " 'specifically',\n",
       " ',',\n",
       " 'we',\n",
       " 'will',\n",
       " 'look',\n",
       " 'at',\n",
       " 'the',\n",
       " 'three',\n",
       " 'main',\n",
       " 'types',\n",
       " 'of',\n",
       " 'token',\n",
       " '##izers',\n",
       " 'used',\n",
       " 'in',\n",
       " '[UNK]',\n",
       " 'transform',\n",
       " '##ers',\n",
       " ':',\n",
       " 'by',\n",
       " '##te',\n",
       " '-',\n",
       " 'pair',\n",
       " 'enc',\n",
       " '##oding',\n",
       " '(',\n",
       " 'bp',\n",
       " '##e',\n",
       " '),',\n",
       " 'word',\n",
       " '##piece',\n",
       " ',',\n",
       " 'and',\n",
       " 'sentence',\n",
       " '##piece',\n",
       " ',',\n",
       " 'and',\n",
       " 'show',\n",
       " 'examples',\n",
       " 'of',\n",
       " 'which',\n",
       " 'token',\n",
       " '##izer',\n",
       " 'type',\n",
       " 'is',\n",
       " 'used',\n",
       " 'by',\n",
       " 'which',\n",
       " 'model',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpiece_tokenizer.encode(text).tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordpiece_tokenizer.save(\"tokenizers/babylm_10M_wordpiece.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SentencePiece Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import SentencePieceBPETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_tokenizer = SentencePieceBPETokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokenizer(vocabulary_size=0, model=SentencePieceBPE, unk_token=<unk>, replacement=▁, add_prefix_space=True, dropout=None)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tokenizers.normalizers.NFKC at 0x7fc946831630>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_tokenizer.normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tokenizers.pre_tokenizers.Metaspace at 0x7fc946839e70>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_tokenizer.pre_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tokenizers.decoders.Metaspace at 0x7fc972f94780>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_tokenizer.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_tokenizer.post_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence_tokenizer.train(files=paths, \n",
    "    vocab_size=30_000,\n",
    "    min_frequency=5,\n",
    "    show_progress=True,\n",
    "    limit_alphabet=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=135, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁As',\n",
       " '▁we',\n",
       " '▁saw',\n",
       " '▁in',\n",
       " '▁the',\n",
       " '▁prep',\n",
       " 'ro',\n",
       " 'cess',\n",
       " 'ing',\n",
       " '▁tut',\n",
       " 'orial',\n",
       " ',',\n",
       " '▁token',\n",
       " 'izing',\n",
       " '▁a',\n",
       " '▁text',\n",
       " '▁is',\n",
       " '▁spl',\n",
       " 'itting',\n",
       " '▁it',\n",
       " '▁into',\n",
       " '▁words',\n",
       " '▁or',\n",
       " '▁sub',\n",
       " 'word',\n",
       " 's,',\n",
       " '▁which',\n",
       " '▁then',\n",
       " '▁are',\n",
       " '▁converted',\n",
       " '▁to',\n",
       " '▁id',\n",
       " 's',\n",
       " '▁through',\n",
       " '▁a',\n",
       " '▁look',\n",
       " '-up',\n",
       " '▁table.',\n",
       " '▁Con',\n",
       " 'ver',\n",
       " 'ting',\n",
       " '▁words',\n",
       " '▁or',\n",
       " '▁sub',\n",
       " 'word',\n",
       " 's',\n",
       " '▁to',\n",
       " '▁id',\n",
       " 's',\n",
       " '▁is',\n",
       " '▁straight',\n",
       " 'for',\n",
       " 'ward,',\n",
       " '▁so',\n",
       " '▁in',\n",
       " '▁this',\n",
       " '▁summ',\n",
       " 'ary,',\n",
       " '▁we',\n",
       " '▁will',\n",
       " '▁focus',\n",
       " '▁on',\n",
       " '▁spl',\n",
       " 'itting',\n",
       " '▁a',\n",
       " '▁text',\n",
       " '▁into',\n",
       " '▁words',\n",
       " '▁or',\n",
       " '▁sub',\n",
       " 'word',\n",
       " 's',\n",
       " '▁(i.e.',\n",
       " '▁token',\n",
       " 'izing',\n",
       " '▁a',\n",
       " '▁text',\n",
       " ').',\n",
       " '▁More',\n",
       " '▁specific',\n",
       " 'ally,',\n",
       " '▁we',\n",
       " '▁will',\n",
       " '▁look',\n",
       " '▁at',\n",
       " '▁the',\n",
       " '▁three',\n",
       " '▁main',\n",
       " '▁types',\n",
       " '▁of',\n",
       " '▁token',\n",
       " 'iz',\n",
       " 'ers',\n",
       " '▁used',\n",
       " '▁in',\n",
       " '▁',\n",
       " '<unk>',\n",
       " '▁Trans',\n",
       " 'form',\n",
       " 'ers',\n",
       " ':',\n",
       " '▁By',\n",
       " 'te',\n",
       " '-P',\n",
       " 'air',\n",
       " '▁En',\n",
       " 'cod',\n",
       " 'ing',\n",
       " '▁(B',\n",
       " 'P',\n",
       " 'E',\n",
       " '),',\n",
       " '▁Word',\n",
       " 'P',\n",
       " 'ie',\n",
       " 'ce,',\n",
       " '▁and',\n",
       " '▁Sent',\n",
       " 'ence',\n",
       " 'P',\n",
       " 'ie',\n",
       " 'ce,',\n",
       " '▁and',\n",
       " '▁show',\n",
       " '▁examples',\n",
       " '▁of',\n",
       " '▁which',\n",
       " '▁token',\n",
       " 'izer',\n",
       " '▁type',\n",
       " '▁is',\n",
       " '▁used',\n",
       " '▁by',\n",
       " '▁which',\n",
       " '▁model.']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_tokenizer.encode(text).tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_tokenizer.save(\"tokenizers/babylm_10M_sentencepiece.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "babylm",
   "language": "python",
   "name": "babylm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
